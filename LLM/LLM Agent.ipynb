{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama_index\n",
    "#!pip install langchain_openai\n",
    "#!pip install langchain_community\n",
    "#!pip install langgraph\n",
    "#!pip install retriever\n",
    "#!pip install chromadb\n",
    "#!pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "import openai\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.core.node_parser import LangchainNodeParser, SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import BaseExtractor, KeywordExtractor, TitleExtractor\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, StorageContext, VectorStoreIndex, load_index_from_storage, Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings---------- #\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=openai.api_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=openai.api_key,\n",
    "    embed_batch_size=100\n",
    ")\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "# Paths for storage\n",
    "DB_DIR = os.getenv(\"DB_DIR\", os.path.join(os.getcwd(), \"docs\", \"chroma\"))\n",
    "INDEX_DIR = os.getenv(\"INDEX_DIR\", os.path.join(os.getcwd(), \"index\"))\n",
    "METADATA_ENRICHMENT_INDEX_DIR = os.getenv(\"METADATA_ENRICHMENT_INDEX_DIR\", os.path.join(os.getcwd(), \"enriched_index\"))\n",
    "\n",
    "# Folder containing the PDF files\n",
    "DATA_FOLDER = \"/Users/iveshe/Library/Mobile Documents/com~apple~CloudDocs/Term 1/BADM 550/CNeutral/LLM/docs\"\n",
    "\n",
    "# Settings end----- #\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": (\n",
    "                    node.metadata[\"document_title\"]\n",
    "                    + \"\\n\"\n",
    "                    + node.metadata[\"excerpt_keywords\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "\n",
    "def metadata_enrichment_index(files=DATA_FOLDER, documents=None):\n",
    "    \"\"\"Create an enriched index with transformations.\"\"\"\n",
    "    nest_asyncio.apply()\n",
    "    extractors = [\n",
    "        TitleExtractor(nodes=5, llm=Settings.llm),\n",
    "        KeywordExtractor(keywords=10, llm=Settings.llm)\n",
    "    ]\n",
    "    transformations = [Settings.node_parser] + extractors\n",
    "    doc_lis = files\n",
    "    docs_nodes = []\n",
    "    pipeline = IngestionPipeline(transformations=transformations)\n",
    "    if documents:\n",
    "        docs_nodes.extend(pipeline.run(documents=docs))\n",
    "    else:\n",
    "        for doc in doc_lis:\n",
    "            docs = SimpleDirectoryReader(input_files=[doc]).load_data()\n",
    "            docs_nodes.extend(pipeline.run(documents=docs))\n",
    "\n",
    "    index = VectorStoreIndex(nodes=docs_nodes, embed_model=Settings.embed_model)\n",
    "    index.storage_context.persist(persist_dir=METADATA_ENRICHMENT_INDEX_DIR)\n",
    "    return index\n",
    "\n",
    "def llama_index_chunk_pdf(files=DATA_FOLDER, index_dir=METADATA_ENRICHMENT_INDEX_DIR):\n",
    "    '''load/create index'''\n",
    "    if not os.path.exists(files):\n",
    "        raise FileNotFoundError(f\"The directory {files} does not exist. Please check the path.\")\n",
    "\n",
    "    file_paths = [os.path.join(files, f) for f in os.listdir(files) if f.endswith('.pdf')]\n",
    "    if not file_paths:\n",
    "        raise FileNotFoundError(f\"No PDF files found in the directory {files}. Please add PDF files to proceed.\")\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "    document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "\n",
    "    if os.path.exists(index_dir):\n",
    "        for file in os.listdir(index_dir):\n",
    "            os.remove(os.path.join(index_dir, file))\n",
    "    else:\n",
    "        os.makedirs(index_dir)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents([document])\n",
    "    index.storage_context.persist(persist_dir=index_dir)\n",
    "    return index\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute chunking process\"\"\"\n",
    "    llama_index_chunk_pdf(files=DATA_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval & Engine\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModel, AutoTokenizer\n",
    "from langchain_core.tools import tool\n",
    "from llama_index.core import Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor#, SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.question_gen.prompts import DEFAULT_SUB_QUESTION_PROMPT_TMPL\n",
    "\n",
    "'''settings'''\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=10,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature= 0.1, timeout=60)\n",
    "Settings.node_parser = node_parser\n",
    "\n",
    "def get_sentence_window_query_engine(sentence_index, similarity_top_k=6):\n",
    "    \"\"\"\n",
    "    Create a sentence window query engine from index.\n",
    "    \"\"\"\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc]\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n",
    "def final_engine(engine, verbose=False):\n",
    "    \"\"\"\n",
    "    Create final query engine with sub-question capability.\n",
    "    \"\"\"\n",
    "    question_gen = LLMQuestionGenerator.from_defaults(\n",
    "        llm=Settings.llm,\n",
    "        prompt_template_str=\"\"\"\n",
    "            Instead of giving a question, always prefix the question\n",
    "            with: 'By first identifying and quoting the most relevant sources, '.\n",
    "            \"\"\" + DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n",
    "    )\n",
    "    return SubQuestionQueryEngine.from_defaults(\n",
    "        query_engine_tools=[\n",
    "            QueryEngineTool(\n",
    "                query_engine=engine,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"docs\",\n",
    "                    description=\"ESG information and portfolio constructions on companies.\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        question_gen=question_gen,\n",
    "        use_async=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "def llama_index_retriever_tool(index_path: str, index_type='sentence', similarity_top_k=6):\n",
    "    \"\"\"\n",
    "   Alows searching and retrieving information from documents using llama-index.\n",
    "    \"\"\"\n",
    "    # Load the index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    \n",
    "    # Create query engine\n",
    "    query_engine = get_sentence_window_query_engine(index, similarity_top_k) if 'sentence' in index_type else index.as_query_engine(similarity_top_k=similarity_top_k)\n",
    "    query_engine = final_engine(query_engine, verbose=True)\n",
    "    \n",
    "    @tool\n",
    "    def engine(query=''):\n",
    "        \"\"\"\n",
    "        RAG query tool.\n",
    "        \"\"\"\n",
    "        response = query_engine.query(query)\n",
    "        print(f'---RAG---:\\n {response}')\n",
    "        return response\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MAX_ATTEMPT = 5\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    attempt_num: int\n",
    "\n",
    "\n",
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\", \"generate_no_ans\"]:\n",
    "   \"\"\"\n",
    "   Determines whether the retrieved documents are relevant to the question.\n",
    "   \"\"\"\n",
    "   print(\"---CHECK RELEVANCE---\")\n",
    "   \n",
    "   try:\n",
    "       messages = state[\"messages\"]\n",
    "       question = messages[0].content\n",
    "       print('Question:', question)\n",
    "       \n",
    "       # Use llama-index for retrieval if index_path provided\n",
    "       if \"index_path\" in state:\n",
    "           retriever = llama_index_retriever_tool(\n",
    "               index_path=state[\"index_path\"],\n",
    "               index_type='sentence',\n",
    "               similarity_top_k=10  # Increased for better coverage\n",
    "           )\n",
    "           \n",
    "           # Get retrieval results\n",
    "           retrieval_response = retriever(question)\n",
    "           docs = str(retrieval_response)\n",
    "           \n",
    "           # Normalize text for matching\n",
    "           question_lower = question.lower()\n",
    "           docs_lower = docs.lower()\n",
    "           \n",
    "           # Extract year and check for temporal+numeric relevance \n",
    "           import re\n",
    "           year_match = re.search(r'20\\d{2}', question)\n",
    "           if year_match:\n",
    "               year = year_match.group()\n",
    "               has_year = year in docs_lower\n",
    "               has_numbers = bool(re.search(r'(?:rm|myr|rp)?\\s*\\d+(?:\\.\\d+)?(?:\\s*(?:million|m|billion|b))?', docs_lower))\n",
    "               \n",
    "               if has_year and has_numbers:\n",
    "                   print(\"---DECISION: DOCS RELEVANT (Contains Year and Numbers)---\")\n",
    "                   print(\"docs:\")\n",
    "                   print(docs)\n",
    "                   return \"generate\"\n",
    "               \n",
    "       else:\n",
    "           docs = messages[-1].content\n",
    "           \n",
    "       print(\"Retrieved docs:\", docs)\n",
    "       \n",
    "       # Grade relevance using LLM\n",
    "       class grade(BaseModel):\n",
    "           binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "       \n",
    "       model = ChatOpenAI(temperature=0.1, model=MODEL_NAME, streaming=True)\n",
    "       llm_with_tool = model.with_structured_output(grade)\n",
    "       \n",
    "       prompt = PromptTemplate(\n",
    "           template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question about financial and ESG data. \\n \n",
    "           Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "           Here is the user question: {question} \\n\n",
    "           \n",
    "           For financial questions, grade the document as relevant if it contains:\n",
    "           1. Specific numeric values (like revenue, profit, etc.) for the requested time period\n",
    "           2. Financial figures with currency indicators (USD, MYR, RM, etc.)\n",
    "           3. Year-specific financial information that matches the question\n",
    "           4. Comparative financial data between years\n",
    "           \n",
    "           The document should be considered relevant even if it needs some interpretation \n",
    "           (e.g., if asking about 2023 and document mentions 'FY2023' or 'current year').\n",
    "           \n",
    "           If the document only states that information is not found or not available, grade it as not relevant.\n",
    "           Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\",\n",
    "           input_variables=[\"context\", \"question\"],\n",
    "       )\n",
    "       chain = prompt | llm_with_tool\n",
    "\n",
    "       scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "       score = scored_result.binary_score\n",
    "\n",
    "       # Return decision based on score and attempt count\n",
    "       if score == \"yes\":\n",
    "           print(\"---DECISION: DOCS RELEVANT---\")\n",
    "           print(\"docs:\")\n",
    "           print(docs)\n",
    "           return \"generate\"\n",
    "       elif state[\"attempt_num\"] < MAX_ATTEMPT:\n",
    "           print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "           print(score)\n",
    "           print(\"docs:\")\n",
    "           print(docs)\n",
    "           return \"rewrite\"\n",
    "       else:\n",
    "           print(\"---DECISION: DOCS NOT RELEVANT, MAX_ATTEMPT achieved---\")\n",
    "           print(score)\n",
    "           print(\"docs:\")\n",
    "           print(docs)\n",
    "           return \"generate_no_ans\"\n",
    "           \n",
    "   except Exception as e:\n",
    "       print(f\"Error in grade_documents: {str(e)}\")\n",
    "       raise\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def agent_with_tools(tools):\n",
    "    def agent(state):\n",
    "        \"\"\"\n",
    "        Invokes the agent model to generate a response based on the current state. Given\n",
    "        the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "        \"\"\"\n",
    "        print(\"---CALL AGENT---\")\n",
    "        messages = state[\"messages\"]\n",
    "        if not state.get(\"attempt_num\"):\n",
    "            state[\"attempt_num\"] = 0  # Initialize attempt number\n",
    "        model = ChatOpenAI(temperature=0.1, streaming=True, model=MODEL_NAME)\n",
    "        model = model.bind_tools(tools)\n",
    "        response = model.invoke(messages)\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\"messages\": [response], \"attempt_num\": state[\"attempt_num\"]}\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0.1, model=MODEL_NAME, streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response], \"attempt_num\": state[\"attempt_num\"]+1}\n",
    "\n",
    "def generate_no_ans(state):\n",
    "    \"\"\"\n",
    "    Generate response when no answer found\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE_NO_ANS---\")\n",
    "    return {\"messages\": [\"No Relevant Info found in the documents\"], \"attempt_num\": 0}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "    \n",
    "    print(\"Question:\", question)\n",
    "    print(\"Last Message:\", last_message)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0.1, streaming=True)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response], \"attempt_num\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "def build_workflow(vecdb):\n",
    "    retriever_tool = llama_index_retriever_tool(vecdb)\n",
    "    tools = [retriever_tool]\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.attempt_num = 0\n",
    "\n",
    "    # Define the nodes we will cycle between\n",
    "    workflow.add_node(\"agent\", agent_with_tools(tools))  # agent\n",
    "    retrieve = ToolNode(tools)\n",
    "    workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "    workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "    workflow.add_node(\"generate_no_ans\", generate_no_ans)  #  Generating a response after we know no document is relevant\n",
    "    workflow.add_node(\"generate\", generate)  # Generating a response after we know the documents are relevant\n",
    "    # Call agent node to decide to retrieve or not\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "\n",
    "    # Decide whether to retrieve\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        # Assess agent decision\n",
    "        tools_condition,\n",
    "        {\n",
    "            # Translate the condition outputs to nodes in our graph\n",
    "            \"tools\": \"retrieve\",\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Edges taken after the `action` node is called.\n",
    "    workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    workflow.add_edge(\"generate_no_ans\", END)\n",
    "    workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "    # Compile\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: What's Public Shareholders' Share for SOL?\n",
      "---CALL AGENT---\n",
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[docs] Q: By first identifying and quoting the most relevant sources, what is the percentage of public shareholders' share for SOL?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[docs] Q: By first identifying and quoting the most relevant sources, what are the recent changes in the ownership structure of SOL?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[docs] Q: By first identifying and quoting the most relevant sources, how does SOL's public shareholder percentage compare to industry standards?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[docs] A: The provided information does not include specific data regarding SOL's public shareholder percentage or a comparison to industry standards. Therefore, it is not possible to answer the query based on the available context.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[docs] A: The percentage of public shareholders' share for Shui On Land (SOL) is 43.77%. This is stated as follows: \"Public Shareholders 43.77%.\"\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[docs] A: The provided information does not contain any details regarding recent changes in the ownership structure of Shui On Land (SOL). It primarily focuses on sustainability initiatives, risk management, stakeholder engagement, and environmental performance. For information on ownership structure, additional sources would be required.\n",
      "\u001b[0m---RAG---:\n",
      " The percentage of public shareholders' share for Shui On Land (SOL) is 43.77%.\n",
      "---CHECK RELEVANCE---\n",
      "Question: What's Public Shareholders' Share for SOL?\n",
      "Retrieved docs: The percentage of public shareholders' share for Shui On Land (SOL) is 43.77%.\n",
      "---DECISION: DOCS RELEVANT---\n",
      "docs:\n",
      "The percentage of public shareholders' share for Shui On Land (SOL) is 43.77%.\n",
      "---GENERATE---\n",
      "Question: What's Public Shareholders' Share for SOL?\n",
      "Last Message: content=\"The percentage of public shareholders' share for Shui On Land (SOL) is 43.77%.\" name='engine' id='d2175f2e-3dec-4fcf-8c41-d74a7f613f75' tool_call_id='call_gjyykRV68ue8mlDYycAyGckk'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Gradio Integration\n",
    "def get_answer_func(graph):\n",
    "    def get_answer_chat(question, history):\n",
    "        print(\"User question:\", question)\n",
    "        result = graph.invoke(\n",
    "            {\"messages\": [HumanMessage(content=question)]},\n",
    "            config={\"configurable\": {\"thread_id\": 42, \"max_attempt\": 5}}\n",
    "        )\n",
    "        response = result[\"messages\"][-1].content\n",
    "        history.append((question, response))\n",
    "        return response\n",
    "    return get_answer_chat\n",
    "\n",
    "index_path = os.path.join(os.getcwd(), \"enriched_index\")\n",
    "graph = build_workflow(index_path)\n",
    "\n",
    "gr.ChatInterface(\n",
    "    get_answer_func(graph),\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    title=\"Agent\",\n",
    "    description=\"Ask me any question\",\n",
    "    theme=\"ocean\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
